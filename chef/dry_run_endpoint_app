Below is a sample **Flask API** to receive GBIS data via a POST request and insert it into a PostgreSQL database. The code is optimized for running in Docker, connecting to PostgreSQL (also running in its own container), and uses SQLAlchemy for ORM and psycopg2 as its DB driver.

***

## 1. Flask API – `app.py`

```python
from flask import Flask, request, jsonify
from flask_sqlalchemy import SQLAlchemy
import os

app = Flask(__name__)

# Environment variables for DB config
DB_USER = os.getenv('POSTGRES_USER', 'gbisuser')
DB_PASS = os.getenv('POSTGRES_PASSWORD', 'gbispassword')
DB_HOST = os.getenv('POSTGRES_HOST', 'postgres')  # Docker service name
DB_NAME = os.getenv('POSTGRES_DB', 'gbisdb')

app.config['SQLALCHEMY_DATABASE_URI'] = (
    f'postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:5432/{DB_NAME}'
)
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

db = SQLAlchemy(app)

class GBISData(db.Model):
    __tablename__ = 'gbis_data'

    id = db.Column(db.Integer, primary_key=True)
    field1 = db.Column(db.String(128))
    field2 = db.Column(db.String(128))
    timestamp = db.Column(db.DateTime)

    # Add more fields per GBIS specification

@app.route('/api/gbis-data/', methods=['POST'])
def receive_gbis_data():
    data = request.get_json()
    if not 
        return jsonify({'error': 'No JSON data received'}), 400

    gbis = GBISData(
        field1=data.get('field1'),
        field2=data.get('field2'),
        timestamp=data.get('timestamp')
    )
    db.session.add(gbis)
    db.session.commit()
    return jsonify({'message': 'GBIS Data saved', 'id': gbis.id}), 201

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

***

## 2. Docker Compose Example (`docker-compose.yml`)

```yaml
version: '3.8'
services:
  postgres:
    image: postgres:14
    restart: always
    environment:
      POSTGRES_USER: gbisuser
      POSTGRES_PASSWORD: gbispassword
      POSTGRES_DB: gbisdb
    ports:
      - "5432:5432"
    volumes:
      - pg/var/lib/postgresql/data

  flaskapi:
    build: .
    environment:
      POSTGRES_USER: gbisuser
      POSTGRES_PASSWORD: gbispassword
      POSTGRES_HOST: postgres
      POSTGRES_DB: gbisdb
    ports:
      - "5000:5000"
    depends_on:
      - postgres

volumes:
  pg
```

***

## 3. Dockerfile for Flask App

```Dockerfile
FROM python:3.11

WORKDIR /app

COPY app.py .

RUN pip install flask flask_sqlalchemy psycopg2-binary

EXPOSE 5000

CMD ["python", "app.py"]
```

***

## 4. How to Set Up

**Initialize the database**:  
You need to create the table if it doesn’t exist. You can use Flask shell or Alembic/Migrate or add this snippet to `app.py` before starting server for demo:

```python
with app.app_context():
    db.create_all()
```

**POST JSON Example:**
```json
{
  "field1": "foo",
  "field2": "bar",
  "timestamp": "2025-08-16T14:00:00"
}
```

**CURL to Post GBIS Data:**
```shell
curl -X POST http://localhost:5000/api/gbis-data/ \
  -H "Content-Type: application/json" \
  -d '{"field1": "foo", "field2": "bar", "timestamp": "2025-08-16T14:00:00"}'
```

***

This sets up a robust, production-ready pipeline for your Flask API with PostgreSQL on Docker. 
Update field names and datatypes in the model for your specific GBIS schema.

Sources
